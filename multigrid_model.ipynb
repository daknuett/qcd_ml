{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3d36cfb-76b7-4acc-af74-09c36daa9913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SharedMemoryMpi:  World communicator of size 1\n",
      "SharedMemoryMpi:  Node  communicator of size 1\n",
      "SharedMemoryMpi: SharedMemoryAllocate 1073741824 MMAP anonymous implementation \n",
      "\n",
      "__|__|__|__|__|__|__|__|__|__|__|__|__|__|__\n",
      "__|__|__|__|__|__|__|__|__|__|__|__|__|__|__\n",
      "__|_ |  |  |  |  |  |  |  |  |  |  |  | _|__\n",
      "__|_                                    _|__\n",
      "__|_   GGGG    RRRR    III    DDDD      _|__\n",
      "__|_  G        R   R    I     D   D     _|__\n",
      "__|_  G        R   R    I     D    D    _|__\n",
      "__|_  G  GG    RRRR     I     D    D    _|__\n",
      "__|_  G   G    R  R     I     D   D     _|__\n",
      "__|_   GGGG    R   R   III    DDDD      _|__\n",
      "__|_                                    _|__\n",
      "__|__|__|__|__|__|__|__|__|__|__|__|__|__|__\n",
      "__|__|__|__|__|__|__|__|__|__|__|__|__|__|__\n",
      "  |  |  |  |  |  |  |  |  |  |  |  |  |  |  \n",
      "\n",
      "\n",
      "Copyright (C) 2015 Peter Boyle, Azusa Yamaguchi, Guido Cossu, Antonin Portelli and other authors\n",
      "\n",
      "This program is free software; you can redistribute it and/or modify\n",
      "it under the terms of the GNU General Public License as published by\n",
      "the Free Software Foundation; either version 2 of the License, or\n",
      "(at your option) any later version.\n",
      "\n",
      "This program is distributed in the hope that it will be useful,\n",
      "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
      "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
      "GNU General Public License for more details.\n",
      "Current Grid git commit hash=9f89486df5e65c873308df23240a3b826c257d76: (HEAD -> feature/gpt, origin/feature/gpt, origin/HEAD) clean\n",
      "\n",
      "Grid : Message : ================================================ \n",
      "Grid : Message : MPI is initialised and logging filters activated \n",
      "Grid : Message : ================================================ \n",
      "Grid : Message : Requested 1073741824 byte stencil comms buffers \n",
      "Grid : Message : MemoryManager::Init() setting up\n",
      "Grid : Message : MemoryManager::Init() cache pool for recent host   allocations: SMALL 8 LARGE 2 HUGE 0\n",
      "Grid : Message : MemoryManager::Init() cache pool for recent device allocations: SMALL 16 LARGE 8 Huge 0\n",
      "Grid : Message : MemoryManager::Init() cache pool for recent shared allocations: SMALL 16 LARGE 8 Huge 0\n",
      "Grid : Message : MemoryManager::Init() Unified memory space\n",
      "\n",
      "=============================================\n",
      "              Initialized GPT                \n",
      "     Copyright (C) 2020 Christoph Lehner     \n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import gpt as g\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"src/\")\n",
    "from qcd_ml.dirac import dirac_wilson_clover\n",
    "from qcd_ml.nn.lptc import v_LPTC_NG\n",
    "from qcd_ml.nn.ptc import v_PTC\n",
    "from qcd_ml.dirac import dirac_wilson_clover\n",
    "\n",
    "from stuff import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3b9a510-4184-4730-8765-292e6b769342",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Smoother_PTC(torch.nn.Module):\n",
    "    def __init__(self, U):\n",
    "        super().__init__()\n",
    "        self.U = U\n",
    "        self.paths = [[]] + [[(mu, 1)] for mu in range(4)] + [[(mu, -1)] for mu in range(4)]\n",
    "        self.l0 = v_PTC(2, 2, self.paths, self.U)\n",
    "        self.l1 = v_PTC(2, 2, self.paths, self.U)\n",
    "        self.l2 = v_PTC(2, 2, self.paths, self.U)\n",
    "        self.l3 = v_PTC(2, 1, self.paths, self.U)\n",
    "\n",
    "    def forward(self, v):\n",
    "        for l in [self.l0, self.l1, self.l2, self.l3]:\n",
    "            v = l.forward(v)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12ed11d3-9666-435e-97ad-80beee23add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multigrid_Model(torch.nn.Module):\n",
    "    def __init__(self, U, mg_setup):\n",
    "        super().__init__()\n",
    "        self.paths_coarse = [[]] + [[(mu, 1)] for mu in range(4)] + [[(3, -1)]]\n",
    "        self.mg_setup = tuple(mg_setup)\n",
    "        self.coarse_model = v_LPTC_NG(1, 1, self.paths_coarse, [2, 2, 2, 4], 4)\n",
    "        self.smoother = Smoother_PTC(U)\n",
    "\n",
    "    def forward(self, v):\n",
    "        v1 = v\n",
    "        v2 = torch.clone(v)\n",
    "        v2_c = v_project(*self.mg_setup, v2[0])\n",
    "        v2_c = self.coarse_model.forward(torch.stack([v2_c]))[0]\n",
    "        v2 = v_prolong(*self.mg_setup, v2_c)\n",
    "        return self.smoother.forward(torch.stack([v1[0], v2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7516d02b-96d9-4127-962a-fda90b007658",
   "metadata": {},
   "outputs": [],
   "source": [
    "mg_setup = torch.load(\"mg_setup.pt\")\n",
    "smoother_weights = torch.load(\"smoother_start_id.pt\")\n",
    "coarse_weights = torch.load(\"coarse.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a73629ee-e48b-446f-8684-83bfdf4d7d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = torch.complex(\n",
    "        torch.randn(8, 8, 8, 16, 4, 3, dtype=torch.double)\n",
    "        , torch.randn(8, 8, 8, 16, 4, 3, dtype=torch.double))\n",
    "\n",
    "U = torch.tensor(np.load(os.path.join(\"test\", \"assets\",\"1500.config.npy\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5e169d0-3b22-4909-91cf-e1f23facee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Multigrid_Model(U, mg_setup)\n",
    "\n",
    "for li, wi in zip([model.smoother.l0\n",
    "                  , model.smoother.l1\n",
    "                  , model.smoother.l2\n",
    "                  , model.smoother.l3], smoother_weights):\n",
    "    li.weights.data = wi\n",
    "\n",
    "for wi in model.coarse_model.parameters():\n",
    "    wi.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe8555cc-4e9b-4985-b267-336a70997ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT :       3.366978 s : Reading /home/knd35666/data/ensembles/ens_001/1500.config/\n",
      "GPT :       3.380393 s : Switching view to [1,1,1,1]/Read\n",
      "GPT :       3.430287 s : Read 0.00109863 GB at 0.0219744 GB/s (0.0667758 GB/s for distribution, 0.0327619 GB/s for reading + checksum, 2.24125 GB/s for checksum, 1 views per node)\n",
      "GPT :       3.445133 s : Read 0.00109863 GB at 0.0767552 GB/s (3.25424 GB/s for distribution, 0.0786294 GB/s for reading + checksum, 0.854283 GB/s for checksum, 1 views per node)\n",
      "GPT :       3.458638 s : Read 0.00109863 GB at 0.083988 GB/s (2.53605 GB/s for distribution, 0.086886 GB/s for reading + checksum, 1.64984 GB/s for checksum, 1 views per node)\n",
      "GPT :       3.471192 s : Read 0.00109863 GB at 0.0930871 GB/s (2.95574 GB/s for distribution, 0.0961382 GB/s for reading + checksum, 3.92839 GB/s for checksum, 1 views per node)\n",
      "GPT :       3.471920 s : Completed reading /home/knd35666/data/ensembles/ens_001/1500.config/ in 0.110262 s\n"
     ]
    }
   ],
   "source": [
    "U_gpt = g.load(\"/home/knd35666/data/ensembles/ens_001/1500.config/\")\n",
    "w_gpt = g.qcd.fermion.wilson_clover(U_gpt, {\"mass\": -0.55,\n",
    "    \"csw_r\": 0.0,\n",
    "    \"csw_t\": 0.0,\n",
    "    \"xi_0\": 1.0,\n",
    "    \"nu\": 1.0,\n",
    "    \"isAnisotropic\": False,\n",
    "    \"boundary_phases\": [1,1,1,1]})\n",
    "\n",
    "w = lambda x: torch.tensor(lattice2ndarray(w_gpt(ndarray2lattice(x.numpy(), U_gpt[0].grid, g.vspincolor))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275c366b-154c-4e5b-8eff-63ba174906c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G [  860| 86.00%] <4.180e-02|  140>\r"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "zero = torch.zeros_like(vec)\n",
    "\n",
    "n_training = 1000\n",
    "check_every = 10\n",
    "\n",
    "loss = np.zeros(n_training)\n",
    "iterations_p = np.zeros(n_training // check_every)\n",
    "ret_p = {\"k\": -1}\n",
    "\n",
    "for t in range(1, n_training+1):\n",
    "    v1 = torch.complex(\n",
    "            torch.randn(8, 8, 8, 16, 4, 3, dtype=torch.double)\n",
    "            , torch.randn(8, 8, 8, 16, 4, 3, dtype=torch.double))\n",
    "    v2 = torch.complex(\n",
    "            torch.randn(8, 8, 8, 16, 4, 3, dtype=torch.double)\n",
    "            , torch.randn(8, 8, 8, 16, 4, 3, dtype=torch.double))\n",
    "\n",
    "    Dinvv2, ret = GMRES_torch(w, v2, v2, eps=1e-3, maxiter=300)\n",
    "\n",
    "    \n",
    "\n",
    "    inp_before_norm = [w(v1), v2]\n",
    "    norms = [norm(vi) for vi in inp_before_norm]\n",
    "    \n",
    "    inp = torch.stack([\n",
    "        vi / ni for vi, ni in zip(inp_before_norm, norms)\n",
    "    ])\n",
    "    # FIXME\n",
    "    outp = torch.stack([\n",
    "        v1 / norms[0], Dinvv2 / norms[0]\n",
    "    ])\n",
    "    \n",
    "\n",
    "    scale = l2norm(inp[0])\n",
    "    inp = inp / scale\n",
    "    outp = outp / scale\n",
    "    \n",
    "    score = complex_mse_loss(model.forward(inp), outp)\n",
    "    print(f\"T [{t:5d}|{t / n_training * 100:6.2f}%] <{score.item():.3e}|{ret_p['k']:5d}>\", end=\"\\r\")\n",
    "    loss[t-1] = score.item()\n",
    "    optimizer.zero_grad()\n",
    "    score.backward()\n",
    "    optimizer.step()\n",
    "    if t % check_every == 0:\n",
    "        print(f\"G [{t:5d}|{t / n_training * 100:6.2f}%] <{score.item():.3e}|{ret_p['k']:5d}>\", end=\"\\r\")\n",
    "        with torch.no_grad():\n",
    "            x_p, ret_p = GMRES_torch(w, vec, vec, prec=lambda v: model.forward(torch.stack([v, vec]))[0], eps=1e-4, maxiter=500)\n",
    "    \n",
    "        iterations_p[t // check_every - 1] = ret_p[\"k\"]\n",
    "    print(f\"  [{t:5d}|{t / n_training * 100:6.2f}%] <{score.item():.3e}|{ret_p['k']:5d}>\", end=\"\\r\")\n",
    "    \n",
    "print()\n",
    "plt.plot(loss)\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58a7857-f115-4fed-a01c-4bbe508813e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss, \".\")\n",
    "plt.title(\"multigrid, start from Id\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"cost\")\n",
    "plt.xlabel(\"training vector no.\")\n",
    "ax = plt.gca().twinx()\n",
    "ax.plot(np.arange(0, n_training, check_every), iterations_p, \"C1.\")\n",
    "ax.set_ylabel(\"iterations required by GMRES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d1581d-4410-4086-87f9-9e74a432f9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, ret = GMRES_torch(w, vec, vec, eps=1e-4, maxiter=500)\n",
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fce38ec-391b-420e-8dda-72c381ee9b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"multigrid_model.01.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c208c63-3020-4e23-a927-19d2ac9e986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for wi in model.coarse_model.parameters():\n",
    "    wi.requires_grad = True\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "zero = torch.zeros_like(vec)\n",
    "\n",
    "n_training = 1000\n",
    "check_every = 10\n",
    "\n",
    "loss = np.zeros(n_training)\n",
    "iterations_p = np.zeros(n_training // check_every)\n",
    "ret_p = {\"k\": -1}\n",
    "\n",
    "for t in range(1, n_training+1):\n",
    "    v1 = torch.complex(\n",
    "            torch.randn(8, 8, 8, 16, 4, 3, dtype=torch.double)\n",
    "            , torch.randn(8, 8, 8, 16, 4, 3, dtype=torch.double))\n",
    "    v2 = torch.complex(\n",
    "            torch.randn(8, 8, 8, 16, 4, 3, dtype=torch.double)\n",
    "            , torch.randn(8, 8, 8, 16, 4, 3, dtype=torch.double))\n",
    "\n",
    "    Dinvv2, ret = GMRES_torch(w, v2, v2, eps=1e-3, maxiter=300)\n",
    "\n",
    "    \n",
    "\n",
    "    inp_before_norm = [w(v1), v2]\n",
    "    norms = [norm(vi) for vi in inp_before_norm]\n",
    "    \n",
    "    inp = torch.stack([\n",
    "        vi / ni for vi, ni in zip(inp_before_norm, norms)\n",
    "    ])\n",
    "    # FIXME\n",
    "    outp = torch.stack([\n",
    "        v1 / norms[0], Dinvv2 / norms[0]\n",
    "    ])\n",
    "    \n",
    "\n",
    "    scale = l2norm(inp[0])\n",
    "    inp = inp / scale\n",
    "    outp = outp / scale\n",
    "    \n",
    "    score = complex_mse_loss(model.forward(inp), outp)\n",
    "    print(f\"T [{t:5d}|{t / n_training * 100:6.2f}%] <{score.item():.3e}|{ret_p['k']:5d}>\", end=\"\\r\")\n",
    "    loss[t-1] = score.item()\n",
    "    optimizer.zero_grad()\n",
    "    score.backward()\n",
    "    optimizer.step()\n",
    "    if t % check_every == 0:\n",
    "        print(f\"G [{t:5d}|{t / n_training * 100:6.2f}%] <{score.item():.3e}|{ret_p['k']:5d}>\", end=\"\\r\")\n",
    "        with torch.no_grad():\n",
    "            x_p, ret_p = GMRES_torch(w, vec, vec, prec=lambda v: model.forward(torch.stack([v, vec]))[0], eps=1e-4, maxiter=500)\n",
    "    \n",
    "        iterations_p[t // check_every - 1] = ret_p[\"k\"]\n",
    "    print(f\"  [{t:5d}|{t / n_training * 100:6.2f}%] <{score.item():.3e}|{ret_p['k']:5d}>\", end=\"\\r\")\n",
    "    \n",
    "print()\n",
    "plt.plot(loss)\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d616aa5-f541-4616-a736-9518c4f2ed83",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss, \".\")\n",
    "plt.title(\"multigrid\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"cost\")\n",
    "plt.xlabel(\"training vector no.\")\n",
    "ax = plt.gca().twinx()\n",
    "ax.plot(np.arange(0, n_training, check_every), iterations_p, \"C1.\")\n",
    "ax.set_ylabel(\"iterations required by GMRES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff6a1d9-09cb-451f-8a79-2eb3f776b2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"multigrid_model.02.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20851784-438b-429f-a04a-262d817f6ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
